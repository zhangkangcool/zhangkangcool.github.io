<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../../img/favicon.ico">
        <title>TensorFlow XLA工作原理 - My Docs</title>
        <link href="../../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../cuda/nvdia_ptx/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../frame/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="1"><a href="#tensorflow-xla" class="nav-link">TensorFlow XLA工作原理</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-bs-level="2"><a href="#jit-just-in-time-compilation" class="nav-link">JIT-Just In Time Compilation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#tf2xla" class="nav-link">TF2XLA</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#xla-hlo" class="nav-link">XLA-HLO</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_1" class="nav-link">代码生成</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-bs-level="2"><a href="#_2" class="nav-link">代码执行</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p>https://zhuanlan.zhihu.com/p/462522388</p>
<h1 id="tensorflow-xla">TensorFlow XLA工作原理</h1>
<p>XLA是TensorFlow图表的编译器，只需添加少量代码，即可明显加速TensorFlow ML模型。下图是谷歌官方提供的XLA性能表现。</p>
<blockquote>
<p>在 Google 基准下的表现
下方是所有 XLA 团队基准模型（在 V100 GPU 上运行）的 TensorFlow 在使用和不使用 XLA 时的相对加速/减速图。</p>
</blockquote>
<p><a href="https://link.zhihu.com/?target=https%3A//www.tensorflowers.cn/t/7338">利用 XLA 将 GPU 性能推向极限www.tensorflowers.cn<img alt="图标" src="https://pic1.zhimg.com/v2-4cd987365d2a249adaca46258b01b568_180x120.jpg" /></a></p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-25fac8f414b97b5bca7cb242e92d9b5f_1440w.jpg" />TensorFlow XLA加速对比</p>
<p>腾讯机智作为国内最专业的AI系统开发团队之一，在TensorFlow内核开发领域积累了丰富经验。本文将介绍XLA的工作原理。</p>
<h2 id="jit-just-in-time-compilation"><strong>JIT-Just In Time Compilation</strong></h2>
<p>我们先来看XLA如何作用于TensorFlow的计算图，下面是一张简单的TensorFlow计算图。</p>
<p><img alt="img" src="https://pic2.zhimg.com/80/v2-cee2904738f5227451adc8f519af2c79_1440w.jpg" /></p>
<p>XLA通过一个TensorFlow的图优化Pass(MarkForCompilation)，在TensorFlow计算图中找到适合被JIT编译的区域。这里我们假设XLA仅支持MatMul和Add。</p>
<p><img alt="img" src="https://pic2.zhimg.com/80/v2-53fa1e861bb511a335618cfa5a8f33b1_1440w.jpg" /></p>
<p>TensorFlow XLA把这个区域定义为一个Cluster，作为一个独立的JIT编译单元，在TensorFlow计算图中通过Node Attribute标示。</p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-c1a14b397de7fb38959e94e8abd8921b_1440w.jpg" /></p>
<p>然后另一个TensorFlow的图优化Pass(EncapsulateSubgraphs)，把cluster转化成TensorFlow的一个Function子图。在原图上用一个Caller节点表示这个Function在原图的位置。</p>
<p><img alt="img" src="https://pic1.zhimg.com/80/v2-a1e1208da191b29613f04455ba5ef6dc_1440w.jpg" /></p>
<p>最后调用TensorFlow的图优化Pass(BuildXlaOps)，把Function节点转化成特殊的Xla节点。</p>
<p><img alt="img" src="https://pic3.zhimg.com/80/v2-ce73e617951bdd1d4ad0cd1f32b6e5ce_1440w.jpg" /></p>
<p>在TensorFlow运行时，运行到XlaCompile时，编译Xla cluster子图，然后把编译完的Executable可执行文件通过XlaExecutableClosure传给XlaRun运行。</p>
<h2 id="tf2xla"><strong>TF2XLA</strong></h2>
<p>TensorFlow运行到XlaCompile节点时。</p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-c4612a1f5a5faec4687c7c1e86bc0c2b_1440w.jpg" /></p>
<p>为了编译这个Function，通过把TensorFlow子图所有的节点翻译成XLA HLO Instruction虚拟指令的形式表达，整个子图也由此转化成XLA HLO Computation。</p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-9a0c3abef95398844dfdadf08ffa48bb_1440w.jpg" /></p>
<h2 id="xla-hlo"><strong>XLA-HLO</strong></h2>
<p>XLA在HLO的图表达上进行图优化。聚合可在同一个GPU Kernel中执行的HLO指令。</p>
<p>HLO图优化前</p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-a3a528936c4655f095ecace4ab7209d3_1440w.jpg" /></p>
<p>HLO图优化后</p>
<p><img alt="img" src="https://pic1.zhimg.com/80/v2-8f38e62642ca6ec55b4a59625a2586dc_1440w.jpg" /></p>
<h2 id="_1"><strong>代码生成</strong></h2>
<p>首先根据虚拟指令分配GPU Stream和显存。</p>
<p>然后IrEmitter把HLO Graph转化成由编译器的中间表达LLVM IR表示的GPU Kernel。LLVM IR如下所示：</p>
<pre><code class="language-python">; ModuleID = 'cluster_36__XlaCompiledKernel_true__XlaNumConstantArgs_1__XlaNumResourceArgs_0_.36'
source_filename = &quot;cluster_36__XlaCompiledKernel_true__XlaNumConstantArgs_1__XlaNumResourceArgs_0_.36&quot;
target datalayout = &quot;e-i64:64-i128:128-v16:16-v32:32-n16:32:64&quot;
target triple = &quot;nvptx64-nvidia-cuda&quot;

@0 = private unnamed_addr constant [4 x i8] zeroinitializer
@1 = private unnamed_addr constant [4 x i8] zeroinitializer
@2 = private unnamed_addr constant [4 x i8] zeroinitializer
@3 = private unnamed_addr constant [4 x i8] zeroinitializer
@4 = private unnamed_addr constant [4 x i8] zeroinitializer
@5 = private unnamed_addr constant [4 x i8] zeroinitializer
@6 = private unnamed_addr constant [4 x i8] zeroinitializer

define void @fusion_1(i8* align 16 dereferenceable(3564544) %alloc2, i8* align 64 dereferenceable(3776) %temp_buf) {
entry:
  %output.invar_address = alloca i64
  %output_y.invar_address = alloca i64
  %arg0.1.raw = getelementptr inbounds i8, i8* %alloc2, i64 0
  %arg0.1.typed = bitcast i8* %arg0.1.raw to [944 x [944 x float]]*
  %fusion.1.raw = getelementptr inbounds i8, i8* %temp_buf, i64 0
  %fusion.1.typed = bitcast i8* %fusion.1.raw to [944 x float]*
  %0 = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !4
  %thread.id.x = sext i32 %0 to i64
  %thread.x = urem i64 %thread.id.x, 944
  %thread.y = udiv i64 %thread.id.x, 944
  %1 = alloca float
  %partial_reduction_result.0 = alloca float
  %2 = load float, float* bitcast ([4 x i8]* @0 to float*)
  %3 = getelementptr inbounds float, float* %partial_reduction_result.0, i32 0
  store float %2, float* %3
  %current_output_linear_index_address = alloca i64
  %4 = alloca i1
  store i1 false, i1* %4
  %5 = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !5
  %block.id.x = sext i32 %5 to i64
  %6 = udiv i64 %block.id.x, 1
  %7 = urem i64 %6, 1
  %8 = udiv i64 %block.id.x, 1
  %9 = urem i64 %8, 8
  %10 = udiv i64 %block.id.x, 8
  %block_origin.0 = mul i64 %10, 1
  %block_origin.1 = mul i64 %9, 1
 ...
</code></pre>
<p>由LLVM生成NVPTX（Nvidia定义的虚拟底层指令表达形式）表达，进而由NVCC生成CuBin可执行代码。PTX如下所示:</p>
<pre><code class="language-python">//
// Generated by LLVM NVPTX Back-End
//

.version 6.0
.target sm_70
.address_size 64

    // .globl   fusion_1

.visible .entry fusion_1(
    .param .u64 fusion_1_param_0,
    .param .u64 fusion_1_param_1
)
.reqntid 944, 1, 1
{
    .reg .pred  %p&lt;9&gt;;
    .reg .f32   %f&lt;25&gt;;
    .reg .b32   %r&lt;31&gt;;
    .reg .b64   %rd&lt;61&gt;;

    ld.param.u64    %rd27, [fusion_1_param_0];
    ld.param.u64    %rd28, [fusion_1_param_1];
    cvta.to.global.u64  %rd29, %rd28;
    cvta.to.global.u64  %rd1, %rd27;
    cvta.global.u64     %rd2, %rd29;
    mov.u32     %r3, %tid.x;
    cvt.u64.u32     %rd3, %r3;
    mov.u32     %r1, %ctaid.x;
    setp.eq.s32     %p1, %r1, 7;
    @%p1 bra    LBB0_4;
    bra.uni     LBB0_1;
LBB0_4:
    selp.b64    %rd4, 48, 128, %p1;
    cvt.u32.u64     %r26, %rd3;
    shl.b64     %rd47, %rd3, 2;
    add.s64     %rd48, %rd47, %rd1;
    add.s64     %rd59, %rd48, 3383296;
    or.b32      %r27, %r26, 845824;
    mul.wide.u32    %rd49, %r27, 582368447;
    shr.u64     %rd50, %rd49, 39;
    cvt.u32.u64     %r28, %rd50;
    mul.lo.s32  %r29, %r28, 945;
    sub.s32     %r2, %r27, %r29;
    mov.f32     %f23, 0f00000000;
...
</code></pre>
<h2 id="_2"><strong>代码执行</strong></h2>
<p>当TensorFlow运行到XlaRun时运行由XlaCompile编译得到的GPU可执行代码（Cubin或PTX）。</p>
<p><img alt="img" src="https://pic1.zhimg.com/80/v2-4384d9301ea98ec281e3d93ace4b1fbc_1440w.jpg" /></p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../../js/base.js"></script>
        <script src="../../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
