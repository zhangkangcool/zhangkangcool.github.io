<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../../../img/favicon.ico">
        <title>XLA编译 - My Docs</title>
        <link href="../../../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../../../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../HLO%20Module/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../flow/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-bs-level="2"><a href="#_1" class="nav-link">一、简介</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<ul>
<li>
<h2 id="_1">一、简介</h2>
</li>
</ul>
<p>XLA的实现目录是tensorflow/compiler,目录结构如下：</p>
<table>
<thead>
<tr>
<th>目录名</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>aot</td>
<td>aot编译相关代码，前面分析的tfcompile_tool代码就在这里</td>
</tr>
<tr>
<td>jit</td>
<td>jit编译相关代码，例如xlalaunch节点的OpKenel、XLA相关的计算图重构，都在这里</td>
</tr>
<tr>
<td>plugin</td>
<td>此模块看起来还没完成，暂不分析</td>
</tr>
<tr>
<td>tests</td>
<td>测试代码</td>
</tr>
<tr>
<td>tf2xla</td>
<td>GraphDef转化为XLA Hlo IR代码</td>
</tr>
<tr>
<td>xla</td>
<td>xla编译器核心代码，HLO IR转化为LLVM IR以及机器码的生成</td>
</tr>
</tbody>
</table>
<p>XLA也是基于LLVM框架开发的，前端的输入是Graph，前端没有将Graph直接转化为LLVM IR，而是转化为了XLA的自定义的中间表示HLO IR.并且为HLO IR设计了一系列的优化器。经过优化的HLO IR接下来会被转化为LLVM IR。</p>
<p><img alt="img" src="https://pic4.zhimg.com/80/v2-05b0c9859a9dbe986450147c8429ba7f_1440w.jpg" /></p>
<p>图2：XLA框架结构</p>
<p>具体来说包含了下列几步：</p>
<ul>
<li>步骤一：由GraphDef创建Graph</li>
<li>步骤二：由tensorflow.Graph编译为HLO IR</li>
<li>步骤三：分析与优化HLO IR</li>
<li>步骤四：由HLO IR转化为llvm IR</li>
<li>步骤五：分析与优化llvm IR</li>
<li>步骤六：生成特定平台的二进制文件</li>
</ul>
<p>xla 的使用分为 AOT和 JIT 两种模式，分别介绍如下。</p>
<p>## 二、AOT模式</p>
<p>对照图2来分析一下AOT编译流程:</p>
<ul>
<li>tensorflow.XlaCompiler.CompilerGraph函数将Graph编译成XLA的中间表示xla.UserComputation.</li>
<li>tensorflow.XlaCompiler.CompilerGraph会创建Executor来执行待编译的Graph，通过绑定设备，为所有节点的创建运算核都是专门设计用来编译的，基类是tensorflow.XlaOpKernel.</li>
<li>tensorflow.XlaOpKernel的子类需要实现Compile接口，通过调用xla.ComputeBuilder接口，将本节点的运算转化为Xla指令(instruction).</li>
<li>xla.ComputeBuilder是对xla.Client的调用封装，通过本接口创建的xla指令(instruction)的操作，最终都会通过xla.Client传输到xla.Service.</li>
<li>xla.Client 和 xla.Service 支持单机模式和分布式模式，实际的编译过程发生在Service端.</li>
<li>AOT编译中，用到的是 xla.CompileOnlyClient 和 xla.CompileOnlyService，分别是xla.Client和xla.Service的实现类.</li>
<li>可以看到，图2中的第一个循环（loop for every node）会为每个node生成一系列xla指令(instruction)，这些指令最终会被加入xla.UserComputation的指令队列里。</li>
<li>接下来xla.CompileOnlyClient.CompileAheadOfTime会将xla.UserComputation编译为可执行代码.</li>
<li>xla.ComputationTracker.BuildHloModule函数会将所有的xla.UserComputation转化为xla.HloComputation，并为之创建xla.HloModule.</li>
<li>至此，Graph 到 HLO IR 的转化阶段完成。</li>
<li>HLO IR进入后续的编译过程，根据平台调用不同平台的具体编译器实现类，这里我们以xla.CpuComiler为例来分析.</li>
<li>xla.CpuComiler的输入是xla.HloModule，首先会调用RunHloPasses创建HloPassPipeline，添加并运行一系列的HloPass.</li>
<li>每一个HloPass都实现了一类HLO指令优化逻辑。通常也是我们比较关心的逻辑所在，包含单不限于图中列举出来的
    xla.AlebraicSimplifier(代数简化),xla.HloConstantFolding（常量折叠）,xla.HloCSE（公共表达式消除）等。</li>
<li>HloPassPipeline优化HLO IR之后，将创建xla.cpu.IrEmitter，进入图2中的第三个循环处理逻辑(loop for every computation of module)：将xla.HloModule中的每个xla.HloComputation转化为llvm IR表示，并创建对应的llvm.Module.</li>
<li>至此，Hlo IR 到 llvm IR的转化阶段完成，后面进入llvm IR的处理阶段。</li>
<li>创建xla.cpu.CompilerFunctor将llvm IR转化为最终的可执行机器代码llvm.object.ObjectFile.中间会调用一系列的llvm ir pass对llvm ir进行优化处理。</li>
<li>至此，llvm ir到可执行机器码的转化阶段完成。</li>
</ul>
<p><img alt="img" src="https://pic1.zhimg.com/80/v2-6f1392271f7397f53495e16fa30efd64_1440w.jpg" /></p>
<p>## 三、JIT</p>
<p>JIT对比AOT来说，过程比较类似，略过共同的部分，我们来分析一下：</p>
<ul>
<li>JIT调用方式的入口在运算核tensorflow.XlaLocalLaunchOp.Compute，tensorflow.XlaLocalLaunchOp是连接外部Graph的Executor和内部JIT调用的桥梁。</li>
<li>如果被调用的计算图缓存不命中，则会调用xla.XlaCompile进行实际的编译。</li>
<li>编译过程类似AOT，不同之处主要在于：首先这次调用的Client和Service的实现类是xla.LocalClient和xla.LocalService；其次，llvm ir到机器码的编译过程，这次是通过xla.cpu.SimpleOrcJIT完成的，它将llvm ir编译为可执行代码，并可被立即调用。</li>
<li>可执行机器码后续会被封装为xla.LocalExecutale</li>
<li>调用xla.LocalExecutable的如后函数Run.</li>
</ul>
<p><img alt="img" src="https://pic2.zhimg.com/80/v2-b79d7e0e93e6f23e56de939039b712c9_1440w.jpg" /></p>
<p>## 四、通过指定环境变量编译</p>
<p>例子代码及运行命令(注释):</p>
<p>```python
  #!/usr/bin/env python
  # -<em>- coding: utf-8 -</em>-
  # export TF_CPP_MIN_VLOG_LEVEL=1
  # env XLA_FLAGS="--xla_dump_to=tmp/hlo --xla_dump_hlo_as_dot" 
  # TF_XLA_FLAGS="--tf_xla_auto_jit=2 --tf_xla_clustering_debug" 
  # TF_DUMP_GRAPH_PREFIX="tmp/hlo" python xla.py</p>
<p>import tensorflow as tf</p>
<p>jit_scope = tf.contrib.compiler.jit.experimental_jit_scope</p>
<p>x = tf.placeholder(tf.float32, name="x_hold")
  y = tf.placeholder(tf.float32, name="y_hold")
  z = tf.placeholder(tf.float32, name="z_hold")</p>
<p>with jit_scope():
      x_y_sum = tf.add(x, y, name="x_y_sum")
      x_y_z_sum = tf.add(x_y_sum, z, name="x_y_z_sum")</p>
<pre><code>  sess = tf.Session()

  result = sess.run(x_y_z_sum, {x: [1.0, 2.0, 3.0, 4.0],
                                y: [1.0, 2.0, 3.0, 4.0],
                                z: [1.0, 2.0, 3.0, 4.0]})
</code></pre>
<p>print(result)</p>
<p># 输出
  # [ 3.  6.  9. 12.]
  # 和大量中间文件： .pbtxt, .ll, .s
  ```</p>
<p>运行命令后，会生成一对.pbtxt, .ll, , .dot, .s的中间文件，包含了中间优化过程和最终的汇编文件。</p>
<p>## 五、使用独立工具链完成编译</p>
<p>### 5.1 编译llvm</p>
<p><code>bash
  $ git clone https://github.com/llvm/llvm-project.git
  $ (cd llvm-project &amp;&amp; git checkout $(cat build_tools/llvm_version.txt))
  $ build_tools/build_mlir.sh ${PWD}/llvm-project/ ${PWD}/llvm-build
  $ mkdir build &amp;&amp; cd build
  $ cmake .. -GNinja \
     -DLLVM_ENABLE_LLD=ON \
     -DCMAKE_BUILD_TYPE=Release \
     -DLLVM_ENABLE_ASSERTIONS=On \
     -DMLIR_DIR=${PWD}/../llvm-build/lib/cmake/mlir
  $ ninja check-mlir-hlo</code></p>
<p>### 5.2 编译tfcompile</p>
<blockquote>
<p><a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/xla/tfcompile">https://www.tensorflow.org/xla/tfcompile</a></p>
</blockquote>
<p>TensorFlow 计算图通常由 TensorFlow 运行时执行。在执行计算图中的每个节点时，均会产生一定的运行时开销。这也会导致二进制文件更大，因为除了计算图本身以外，还需包含 TensorFlow 运行时代码。由 <code>tfcompile</code> 生成的可执行代码不会使用 TensorFlow 运行时，而仅仅依赖于计算实际使用的内核。</p>
<p>编译器基于 XLA 框架构建。<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/code/tensorflow/compiler/">tensorflow/compiler</a> 下提供了用于将 TensorFlow 桥接到 XLA 框架的代码。</p>
<p><strong>编译：</strong></p>
<p><code>text
  # bazel build tensorflow/compiler/xla/tools:interactive_graphviz 
  bazel build tensorflow/compiler/aot:tfcompile</code></p>
<p><strong>运行：</strong></p>
<p><code>bash
  # bazel-bin/tensorflow/compiler/xla/tools/interactive_graphviz --hlo_text=/tmp/foo/module_0000.before_optimizations.txt --platform=cpu 
  tfcompile --graph=mygraph.pb --config=myfile.pbtxt --cpp_class="mynamespace::MyComputation"</code></p>
<p>### 5.3 编译tf-opt和tf-mlir-translate</p>
<p><strong>（1）克隆仓库</strong></p>
<p><code>text
  $ git clone git@github.com:tensorflow/tensorflow.git</code></p>
<p><strong>（2）设置LLVM变量</strong></p>
<p>此处注意应当把<code>llvm-project</code>仓库回退到相应版本，具体回退版本要参考<code>tensorflow/third_party</code>路径下的commit ID</p>
<p><code>text
  $ (cd llvm-project &amp;&amp; git checkout $(cat build_tools/llvm_version.txt))
  $ LLVM_SRC=&lt;path to llvm-project&gt;/</code></p>
<p>（3）生成workspace文件</p>
<p><code>text
  $ echo 'workspace(name = "llvm-project")' &gt; $LLVM_SRC/WORKSPACE</code></p>
<p>（4）拷贝bazel BUILD文件</p>
<p><code>python
  cd tensorflow 
  cp third_party/llvm/llvm.autogenerated.BUILD LLVM_SRC/llvm/BUILD 
  cp third_party/mlir/BUILD LLVM_SRC/mlir 
  cp third_party/mlir/test.BUILD LLVM_SRC/mlir/test/BUILD</code></p>
<p>（5）构建<code>tf-opt</code> &amp; <code>tf-mlir-translate</code></p>
<p><code>text
  $ bazel build --override_repository=llvm-project=$LLVM_SRC -c opt tensorflow/compiler/mlir:tf-opt 
  $ bazel build --override_repository=llvm-project=$LLVM_SRC -c opt tensorflow/compiler/mlir:tf-mlir-translate</code></p>
<p>构建完成后，工具链在<code>tensorflow/bazel-bin/tensorflow/compiler/mlir</code>路径下。</p>
<p>如果出现报错：</p>
<p><code>text
  error running 'git fetch origin refs/heads/*:refs/remotes/origin/* refs/tags/*:refs/tags/*' while working with @io_bazel_rules_docker: Timed out</code></p>
<p>解决方法：在<code>tensorflow/WORKSPACE</code>中添加：</p>
<p><code>text
  http_archive(name = "io_bazel_rules_docker", sha256 = "aed1c249d4ec8f703edddf35cbe9dfaca0b</code></p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../../../js/base.js"></script>
        <script src="../../../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
