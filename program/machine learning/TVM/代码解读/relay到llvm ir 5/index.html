<!DOCTYPE html>
<html lang="en" data-bs-theme="light">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../../../img/favicon.ico">
        <title>Relay到llvm ir 5 - My Docs</title>
        <link href="../../../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../../../css/fontawesome.min.css" rel="stylesheet">
        <link href="../../../../../css/brands.min.css" rel="stylesheet">
        <link href="../../../../../css/solid.min.css" rel="stylesheet">
        <link href="../../../../../css/v4-font-face.min.css" rel="stylesheet">
        <link href="../../../../../css/base.css" rel="stylesheet">
        <link id="hljs-light" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" >
        <link id="hljs-dark" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github-dark.min.css" disabled>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script>hljs.highlightAll();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../../../..">My Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbar-collapse" aria-controls="navbar-collapse" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">

                    <ul class="nav navbar-nav ms-md-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-bs-toggle="modal" data-bs-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../relay%E5%88%B0llvm%20ir%204/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../../cuda/CUDA%E5%AE%9E%E4%BE%8B/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-bs-toggle="collapse" data-bs-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-body-tertiary">
        <ul class="nav flex-column">
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p>代码行数可能和现在的有些出入，目前来讲改动还不是很剧烈，无伤大雅。</p>
<p>现在是属于ScheduleGetter的Create时间。</p>
<p>还是拆为若干逻辑部分来看，第一部分是对参数的处理。</p>
<pre><code class="language-text">CachedFunc Create(const Function&amp; prim_func) {
    auto cache_node = make_object&lt;CachedFuncNode&gt;();
    cache_node-&gt;target = target_;
    for (Var param : prim_func-&gt;params) {
      Array&lt;tvm::te::Tensor&gt; inputs;
      if (const auto* ttype = param-&gt;checked_type().as&lt;TensorTypeNode&gt;()) {
        tvm::te::Tensor tensor = tvm::te::placeholder(GetShape(ttype-&gt;shape), ttype-&gt;dtype);
        cache_node-&gt;inputs.push_back(tensor);
        inputs.push_back(tensor);
      } else {
        // flatten tuple of tensor type.
        const auto* tuple_type = param-&gt;type_as&lt;TupleTypeNode&gt;();
        for (Type field : tuple_type-&gt;fields) {
          const auto* ttype = field.as&lt;TensorTypeNode&gt;();
          // TODO(@icemelon): Allow recursive tuple
          CHECK(ttype != nullptr);
          tvm::te::Tensor tensor = tvm::te::placeholder(GetShape(ttype-&gt;shape), ttype-&gt;dtype);
          cache_node-&gt;inputs.push_back(tensor);
          inputs.push_back(tensor);
        }
      }
      memo_[param] = inputs;
    }
    ...
}
</code></pre>
<p>上回说到CreateSschedule将relay IR转为Tensor和Operation构成的数据流图，这里就是把Function各参数输入转换为placeholder这玩意，placeholder表示一个占位符，表示还没有确定的Tensor，约束了该Tensor的形状和类型。看<dive into deep learning compiler>里经常出现te.placeholder就是生成出玩意的实例。处理tuple参数的方法是在else块中将他们都flatten。</p>
<pre><code class="language-text"> CachedFunc Create(const Function&amp; prim_func) {
    ...
    readable_name_stream_ &lt;&lt; &quot;fused&quot;;
    cache_node-&gt;outputs = this-&gt;VisitExpr(prim_func-&gt;body);
    auto candidate_name = readable_name_stream_.str();
    constexpr static size_t kMaxFuncNameLength = 80;
    if (candidate_name.size() &gt; kMaxFuncNameLength) {
      std::stringstream truncated_name;
      truncated_name &lt;&lt; candidate_name.substr(0, kMaxFuncNameLength);
      truncated_name &lt;&lt; &quot;_&quot; &lt;&lt; std::hash&lt;std::string&gt;{}(candidate_name) &lt;&lt; &quot;_&quot;;
      candidate_name = truncated_name.str();
    }
    cache_node-&gt;func_name = candidate_name;

    CHECK(master_op_.defined());
    ...
}
</code></pre>
<p>这是第二部分，又出现了VisitExpr，这就是把relay不同类型表达式转换成Tensor &amp; Operation的主要逻辑了。最后会用cache_node-&gt;outputs来保存转换后该函数所有输出的tensor(s)，用作之后创建Schedule，有兴趣可以瞅瞅对各种Relay Expr都是怎么处理的。</p>
<p>俺这里还是关注Array<te::Tensor> VisitExpr_(const CallNode* call_node)。按打算将这函数切俩半来学习。</p>
<pre><code class="language-text">Array&lt;te::Tensor&gt; VisitExpr_(const CallNode* call_node) final {
    static auto fpattern = Op::GetAttrMap&lt;TOpPattern&gt;(&quot;TOpPattern&quot;);
    static auto flower_call = tvm::runtime::Registry::Get(&quot;relay.backend.lower_call&quot;);
    CHECK(flower_call) &lt;&lt; &quot;relay.backend.lower_call is not registered.&quot;;

    Array&lt;te::Tensor&gt; inputs;
    int count_tuple = 0;
    for (Expr arg : call_node-&gt;args) {
      if (arg-&gt;checked_type().as&lt;TupleTypeNode&gt;()) {
        ++count_tuple;
      }
      for (te::Tensor tensor : VisitExpr(arg)) {
        inputs.push_back(tensor);
      }
    }
    if (count_tuple) {
      CHECK_EQ(call_node-&gt;args.size(), 1U) &lt;&lt; &quot;Only allow function with a single tuple input&quot;;
    }

    CHECK(call_node-&gt;op.as&lt;OpNode&gt;()) &lt;&lt; &quot;Primitive function only allows call into primitive ops&quot;;
    Op op = Downcast&lt;Op&gt;(call_node-&gt;op);

    Array&lt;te::Tensor&gt; outputs;
    OpImplementation impl;
    // Skip fcompute for device copy operators as it is not registered.
    if (op == device_copy_op_) {
      const auto* copy_input = inputs[0].operator-&gt;();
      outputs.push_back(te::Tensor(copy_input-&gt;shape, copy_input-&gt;dtype, te::Operation(), 0));
    } else {
      LoweredOutput lowered_out = (*flower_call)(GetRef&lt;Call&gt;(call_node), inputs, target_);
      outputs = lowered_out-&gt;outputs;
      impl = lowered_out-&gt;implementation;
    }
    ...
}
</code></pre>
<p>这是前半部分代码，对该CallNode的实参们调用VisitExpr，获得实参(们)转换后得的Tensor(s)。还有就是获得存储各Op模式(Pattern)的表的引用，存在fpattern变量种。以及使flower_call成为指向函数lower_call的引用。</p>
<p>fpattern可以认为是kv map，键类型是Op(即relay::Op)，每个实例代表系统已注册的各个relay op。值类型则是枚举类型OpPatternKind的值。OpPatternKind定义在include/tvm/relay/op_attr<em>_</em>types.h:45, 源码如下:</p>
<pre><code class="language-text">enum OpPatternKind {
  // Elementwise operation
  kElemWise = 0,
  // Broadcasting operator, can always map output axis to the input in order.
  // for example :code:`out[i, ax1, j, ax2] = input[i, j]`.
  // Note that the axis need to be in order so transpose is not a bcast operator.
  kBroadcast = 1,
  // Injective operator, can always injectively map output axis to a single input axis.
  // All injective operator can still be safely fused to injective and reduction.
  kInjective = 2,
  // Communicative reduction operator.
  kCommReduce = 3,
  // Complex operation, can still fuse elemwise operations into its output.
  // but cannot chain another complex op
  kOutEWiseFusable = 4,
  // The pattern for tuple nodes. Can fuse into subsequent injective ops,
  // but treated specially
  kTuple = 7,
  // Opaque operation, cannot fuse anything.
  kOpaque = 8
};
</code></pre>
<p>不过寥寥几种，似乎值越大表示复杂度越高。模式的具体作用在后半部分讨论。</p>
<p>而lower_call是一个python函数，定义在<strong>python/tvm/relay/backend/compile_engine.py:229</strong>。作用是将一个调用表达式转换为被调用Op最优的OpImplementation，以及该调用表达式输出的Tensor(s)。回忆上一篇，OpImplementation是平台相关的，如果我们想部署在CPU平台上，总不能生成一个cuda kernel吧...</p>
<p>先判断该Op是否为host &lt;-&gt; device的数据传输操作，如果是的话处理比较简单，就8说了。如果不是，则会调用刚刚获得的lower_call来获取OpImplementation和输出，保存到lowered_out 中，这个lowered_out 可以理解为类型是（OpImplementation， Array<Tensor>）的Tuple。</p>
<p>以下是lower_call的代码。</p>
<pre><code class="language-python3">@tvm._ffi.register_func(&quot;relay.backend.lower_call&quot;)
def lower_call(call, inputs, target):
    &quot;&quot;&quot;Lower the call expression to op implementation and tensor outputs.&quot;&quot;&quot;
    assert isinstance(call.op, tvm.ir.Op)
    op = call.op

    # Prepare the call_node-&gt;checked_type(). For the call node inputs, we ensure that
    # the shape is Int32. Following code ensures the same for the output as well.
    # TODO(@icemelon9): Support recursive tuple
    ret_type = call.checked_type
    if isinstance(ret_type, _ty.TensorType):
        ret_type = _ty.TensorType(get_shape(ret_type.shape), ret_type.dtype)
    elif isinstance(ret_type, _ty.TupleType):
        new_fields = []
        for field in ret_type.fields:
            if isinstance(field, _ty.TensorType):
                new_fields.append(_ty.TensorType(get_shape(field.shape), field.dtype))
            else:
                new_fields.append(field)
        ret_type = _ty.TupleType(new_fields)

    is_dyn = _ty.type_has_any(call.checked_type)
    for arg in call.args:
        is_dyn = is_dyn or _ty.type_has_any(arg.checked_type)

    # check if in the AutoTVM tracing mode, and disable if op is not in wanted list
    env = autotvm.task.TaskExtractEnv.current
    reenable_tracing = False
    if env is not None and env.tracing:
        if env.wanted_relay_ops is not None and op not in env.wanted_relay_ops:
            env.tracing = False
            reenable_tracing = True

    if not is_dyn:
        best_impl, outputs = select_implementation(
            op, call.attrs, inputs, ret_type, target)
        logger.info(&quot;Use implementation %s for op %s&quot;, best_impl.name, op.name)
    else:
        # TODO(@icemelon9): Allow tvm to generate multiple kernels for dynamic shapes.
        #   Currently, we just use the implementation with highest plevel
        best_impl, outputs = select_implementation(
            op, call.attrs, inputs, ret_type, target, use_autotvm=False)

    # re-enable AutoTVM tracing
    if reenable_tracing:
        env.tracing = True
    return LoweredOutput(outputs, best_impl)
</code></pre>
<p>真的很冗长...</p>
<p>ret_type那部分就是构造等价的输出类型，不难理解。</p>
<p>然后就是判断输入/输出的Tensor(们)的类型是不是动态的，结果保存在布尔变量is_dyn中。源码在<strong>src/relay/analysis/<a href="https://link.zhihu.com/?target=http%3A//util.cc%3A423">http://util.cc:423</a></strong>，主体类是IsDynamicVisitor。动态Tensor的判断条件是Tensor的各个维度是否允许为任何值，代码中用AnyNode表示。而我们的简单例子中，is_dyn为假。</p>
<p>下边的AutoTVM相关的我暂时还不是太了解。</p>
<p>因为is_dyn为假，if判断条件成立，便会在调用select_implementation时默认启用autotvm。lower_call最后会将select_implementation的结果以LoweredOutput实例返回。</p>
<p>select_implementation的作用是为op选取最优的OpImplementation，其注释上说，如果启用了AutoTVM则使用相关机制来决策，没有的话就选用最高有优先级的实现。俺将它的代码拆分成常规决策和使用AutoTVM两部分, 以下为第一部分:</p>
<pre><code class="language-text">def select_implementation(op, attrs, inputs, out_type, target, use_autotvm=True):
    all_impls = get_valid_implementations(op, attrs, inputs, out_type, target)

    best_plevel_impl = None
    for impl in all_impls:
        if best_plevel_impl is None or impl.plevel &gt; best_plevel_impl.plevel:
            best_plevel_impl = impl
    if not use_autotvm:
        outs = best_plevel_impl.compute(attrs, inputs, out_type)
        return best_plevel_impl, outs
    ... 
</code></pre>
<p>上回给到select_implementation的第一部分代码，瞅瞅它干了啥。</p>
<p>首先是调用了<strong>get_valid_implementations</strong>获得所有的候选实现。省掉注释后的代码长这样:</p>
<pre><code class="language-text">def get_valid_implementations(op, attrs, inputs, out_type, target):
    fstrategy = op.get_attr(&quot;FTVMStrategy&quot;)
    assert fstrategy is not None, &quot;%s doesn't have FTVMStrategy registered&quot; % op.name
    with target:
        strategy = fstrategy(attrs, inputs, out_type, target)
    analyzer = tvm.arith.Analyzer()
    ret = []
    for spec in strategy.specializations:
        if spec.condition:
            # check if all the clauses in the specialized condition are true
            flag = True
            for clause in spec.condition.clauses:
                clause = analyzer.canonical_simplify(clause)
                if isinstance(clause, tvm.tir.IntImm) and clause.value:
                    continue
                flag = False
                break
            if flag:
                for impl in spec.implementations:
                    ret.append(impl)
        else:
            for impl in spec.implementations:
                ret.append(impl)
    return ret
</code></pre>
<p>在<a href="https://zhuanlan.zhihu.com/p/164931829">第三篇</a>中，已经简要介绍了OpStrategy的角色，结合官方文档，这里不再累述。OpStrategy是存在对应Op的属性表中，所以第一句话是通过传入Op的键为“FTVMStrategy”从属性表中获得对应的OpStrategy。然后遍历OpStrategy包含的所有OpSpecialization实例，如果当前环境满足该实例下的全部条件子句(condition clause)，则把该OpSpecialization实例下的全部实现(<strong>OpImplementation</strong>)加入到<strong>ret</strong>中。ret是元素类型为OpImplementation的数组。最后将ret返回。</p>
<p>Ps: 以后有需要再去探究各个条件从句是啥，这里只管主线剧情就好了。</p>
<p>那么回到select_implementation中。下一步就是遍历刚才获得的ret中的各个实现，选取最“好”的实现，选取的标准是每个实现的plevel值，也就是他们的优先级，这里plevel越大优先级越高。</p>
<p>此时，如果没有启用autotvm的话，就直接调用compute生成输出Tensor(s)，并将Tensor(s)和刚才挑选出的最优实现返回即可。</p>
<p>但很可惜，我们简单例子中要使用autotvm...</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script src="../../../../../js/bootstrap.bundle.min.js"></script>
        <script>
            var base_url = "../../../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../../../js/base.js"></script>
        <script src="../../../../../search/main.js"></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
